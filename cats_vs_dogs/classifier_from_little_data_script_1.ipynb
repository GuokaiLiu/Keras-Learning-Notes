{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'''This script goes along the blog post\n",
    "\"Building powerful image classification models using very little data\"\n",
    "from blog.keras.io.\n",
    "It uses data that can be downloaded at:\n",
    "https://www.kaggle.com/c/dogs-vs-cats/data\n",
    "In our setup, we:\n",
    "- created a data/ folder\n",
    "- created train/ and validation/ subfolders inside data/\n",
    "- created cats/ and dogs/ subfolders inside train/ and validation/\n",
    "- put the cat pictures index 0-999 in data/train/cats\n",
    "- put the cat pictures index 1000-1400 in data/validation/cats\n",
    "- put the dogs pictures index 12500-13499 in data/train/dogs\n",
    "- put the dog pictures index 13500-13900 in data/validation/dogs\n",
    "So that we have 1000 training examples for each class, and 400 validation examples for each class.\n",
    "In summary, this is our directory structure:\n",
    "```\n",
    "data/\n",
    "    train/\n",
    "        dogs/\n",
    "            dog001.jpg\n",
    "            dog002.jpg\n",
    "            ...\n",
    "        cats/\n",
    "            cat001.jpg\n",
    "            cat002.jpg\n",
    "            ...\n",
    "    validation/\n",
    "        dogs/\n",
    "            dog001.jpg\n",
    "            dog002.jpg\n",
    "            ...\n",
    "        cats/\n",
    "            cat001.jpg\n",
    "            cat002.jpg\n",
    "            ...\n",
    "```\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./pictures/cats_and_dogs.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dimensions of our images.\n",
    "img_width, img_height = 150, 150\n",
    "\n",
    "train_data_dir = 'data/train'\n",
    "validation_data_dir = 'data/validation'\n",
    "nb_train_samples = 2000\n",
    "nb_validation_samples = 800\n",
    "epochs = 50\n",
    "batch_size = 16\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    input_shape = (3, img_width, img_height)\n",
    "else:\n",
    "    input_shape = (img_width, img_height, 3)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), input_shape=input_shape))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./pictures/cat_data_augmentation.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this is the augmentation configuration we will use for training\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1. / 255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this is the augmentation configuration we will use for testing:\n",
    "# only rescaling\n",
    "test_datagen = ImageDataGenerator(rescale=1. / 255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n",
      "Found 802 images belonging to 2 classes.\n",
      "Epoch 1/50\n",
      "125/125 [==============================] - 12s - loss: 0.6987 - acc: 0.5290 - val_loss: 0.6645 - val_acc: 0.5887\n",
      "Epoch 2/50\n",
      "125/125 [==============================] - 10s - loss: 0.6740 - acc: 0.6075 - val_loss: 0.6317 - val_acc: 0.6527\n",
      "Epoch 3/50\n",
      "125/125 [==============================] - 11s - loss: 0.6321 - acc: 0.6600 - val_loss: 0.5834 - val_acc: 0.6934\n",
      "Epoch 4/50\n",
      "125/125 [==============================] - 10s - loss: 0.6127 - acc: 0.6675 - val_loss: 0.5779 - val_acc: 0.6845\n",
      "Epoch 5/50\n",
      "125/125 [==============================] - 10s - loss: 0.5913 - acc: 0.6930 - val_loss: 0.5745 - val_acc: 0.6959\n",
      "Epoch 6/50\n",
      "125/125 [==============================] - 10s - loss: 0.5709 - acc: 0.7235 - val_loss: 0.5866 - val_acc: 0.6743\n",
      "Epoch 7/50\n",
      "125/125 [==============================] - 10s - loss: 0.5654 - acc: 0.7120 - val_loss: 0.5676 - val_acc: 0.6781\n",
      "Epoch 8/50\n",
      "125/125 [==============================] - 10s - loss: 0.5530 - acc: 0.7310 - val_loss: 0.5775 - val_acc: 0.6845\n",
      "Epoch 9/50\n",
      "125/125 [==============================] - 10s - loss: 0.5276 - acc: 0.7555 - val_loss: 0.5277 - val_acc: 0.7303\n",
      "Epoch 10/50\n",
      "125/125 [==============================] - 10s - loss: 0.5347 - acc: 0.7465 - val_loss: 0.5264 - val_acc: 0.7112\n",
      "Epoch 11/50\n",
      "125/125 [==============================] - 10s - loss: 0.5324 - acc: 0.7550 - val_loss: 0.5200 - val_acc: 0.7443\n",
      "Epoch 12/50\n",
      "125/125 [==============================] - 10s - loss: 0.5040 - acc: 0.7670 - val_loss: 0.6041 - val_acc: 0.6794\n",
      "Epoch 13/50\n",
      "125/125 [==============================] - 10s - loss: 0.4977 - acc: 0.7720 - val_loss: 0.5408 - val_acc: 0.7417\n",
      "Epoch 14/50\n",
      "125/125 [==============================] - 10s - loss: 0.4957 - acc: 0.7715 - val_loss: 0.6049 - val_acc: 0.7137\n",
      "Epoch 15/50\n",
      "125/125 [==============================] - 10s - loss: 0.4856 - acc: 0.7805 - val_loss: 0.6078 - val_acc: 0.7150\n",
      "Epoch 16/50\n",
      "125/125 [==============================] - 10s - loss: 0.4815 - acc: 0.7765 - val_loss: 0.4820 - val_acc: 0.7761\n",
      "Epoch 17/50\n",
      "125/125 [==============================] - 10s - loss: 0.4698 - acc: 0.7830 - val_loss: 0.4937 - val_acc: 0.7595\n",
      "Epoch 18/50\n",
      "125/125 [==============================] - 11s - loss: 0.4642 - acc: 0.7900 - val_loss: 0.4914 - val_acc: 0.7481\n",
      "Epoch 19/50\n",
      "125/125 [==============================] - 10s - loss: 0.4709 - acc: 0.7865 - val_loss: 0.4907 - val_acc: 0.7659\n",
      "Epoch 20/50\n",
      "125/125 [==============================] - 11s - loss: 0.4593 - acc: 0.7975 - val_loss: 0.5683 - val_acc: 0.7532\n",
      "Epoch 21/50\n",
      "125/125 [==============================] - 10s - loss: 0.4607 - acc: 0.7900 - val_loss: 0.5036 - val_acc: 0.7799\n",
      "Epoch 22/50\n",
      "125/125 [==============================] - 11s - loss: 0.4607 - acc: 0.7955 - val_loss: 0.5309 - val_acc: 0.7494\n",
      "Epoch 23/50\n",
      "125/125 [==============================] - 10s - loss: 0.4569 - acc: 0.7960 - val_loss: 0.5024 - val_acc: 0.7443\n",
      "Epoch 24/50\n",
      "125/125 [==============================] - 10s - loss: 0.4588 - acc: 0.8020 - val_loss: 0.4654 - val_acc: 0.7926\n",
      "Epoch 25/50\n",
      "125/125 [==============================] - 10s - loss: 0.4297 - acc: 0.8045 - val_loss: 0.5946 - val_acc: 0.7366\n",
      "Epoch 26/50\n",
      "125/125 [==============================] - 10s - loss: 0.4383 - acc: 0.8030 - val_loss: 0.4973 - val_acc: 0.7875\n",
      "Epoch 27/50\n",
      "125/125 [==============================] - 11s - loss: 0.4338 - acc: 0.8070 - val_loss: 0.4813 - val_acc: 0.7786\n",
      "Epoch 28/50\n",
      "125/125 [==============================] - 10s - loss: 0.4460 - acc: 0.7975 - val_loss: 0.4707 - val_acc: 0.7697\n",
      "Epoch 29/50\n",
      "125/125 [==============================] - 11s - loss: 0.4215 - acc: 0.8175 - val_loss: 0.6922 - val_acc: 0.7481\n",
      "Epoch 30/50\n",
      "125/125 [==============================] - 10s - loss: 0.4531 - acc: 0.7980 - val_loss: 0.4706 - val_acc: 0.7901\n",
      "Epoch 31/50\n",
      "125/125 [==============================] - 10s - loss: 0.4185 - acc: 0.8140 - val_loss: 0.4789 - val_acc: 0.7888\n",
      "Epoch 32/50\n",
      "125/125 [==============================] - 10s - loss: 0.4250 - acc: 0.8105 - val_loss: 0.4582 - val_acc: 0.8015\n",
      "Epoch 33/50\n",
      "125/125 [==============================] - 10s - loss: 0.4278 - acc: 0.8205 - val_loss: 0.5480 - val_acc: 0.7888\n",
      "Epoch 34/50\n",
      "125/125 [==============================] - 10s - loss: 0.4386 - acc: 0.8110 - val_loss: 0.4817 - val_acc: 0.7939\n",
      "Epoch 35/50\n",
      "125/125 [==============================] - 11s - loss: 0.4316 - acc: 0.8135 - val_loss: 0.5553 - val_acc: 0.7735\n",
      "Epoch 36/50\n",
      "125/125 [==============================] - 11s - loss: 0.4250 - acc: 0.8135 - val_loss: 0.5805 - val_acc: 0.7748\n",
      "Epoch 37/50\n",
      "125/125 [==============================] - 10s - loss: 0.4165 - acc: 0.8115 - val_loss: 0.4861 - val_acc: 0.7913\n",
      "Epoch 38/50\n",
      "125/125 [==============================] - 11s - loss: 0.4292 - acc: 0.8090 - val_loss: 0.6336 - val_acc: 0.7748\n",
      "Epoch 39/50\n",
      "125/125 [==============================] - 10s - loss: 0.4297 - acc: 0.8115 - val_loss: 0.4691 - val_acc: 0.8130\n",
      "Epoch 40/50\n",
      "125/125 [==============================] - 10s - loss: 0.3929 - acc: 0.8295 - val_loss: 0.4732 - val_acc: 0.8028\n",
      "Epoch 41/50\n",
      "125/125 [==============================] - 10s - loss: 0.4078 - acc: 0.8305 - val_loss: 0.5380 - val_acc: 0.7875\n",
      "Epoch 42/50\n",
      "125/125 [==============================] - 10s - loss: 0.4127 - acc: 0.8155 - val_loss: 1.1044 - val_acc: 0.7239\n",
      "Epoch 43/50\n",
      "125/125 [==============================] - 10s - loss: 0.4358 - acc: 0.8175 - val_loss: 0.5035 - val_acc: 0.8079\n",
      "Epoch 44/50\n",
      "125/125 [==============================] - 10s - loss: 0.4193 - acc: 0.8240 - val_loss: 0.4719 - val_acc: 0.8181\n",
      "Epoch 45/50\n",
      "125/125 [==============================] - 10s - loss: 0.4268 - acc: 0.8265 - val_loss: 0.5041 - val_acc: 0.7824\n",
      "Epoch 46/50\n",
      "125/125 [==============================] - 10s - loss: 0.4173 - acc: 0.8215 - val_loss: 0.4842 - val_acc: 0.7710\n",
      "Epoch 47/50\n",
      "125/125 [==============================] - 11s - loss: 0.4107 - acc: 0.8250 - val_loss: 0.5175 - val_acc: 0.7748\n",
      "Epoch 48/50\n",
      "125/125 [==============================] - 10s - loss: 0.4195 - acc: 0.8260 - val_loss: 0.4691 - val_acc: 0.7939\n",
      "Epoch 49/50\n",
      "125/125 [==============================] - 10s - loss: 0.4367 - acc: 0.8085 - val_loss: 0.4732 - val_acc: 0.8015\n",
      "Epoch 50/50\n",
      "125/125 [==============================] - 10s - loss: 0.4339 - acc: 0.8015 - val_loss: 0.5205 - val_acc: 0.7697\n"
     ]
    }
   ],
   "source": [
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    validation_data_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary')\n",
    "\n",
    "model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=nb_train_samples // batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=nb_validation_samples // batch_size)\n",
    "\n",
    "model.save_weights('first_try.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
